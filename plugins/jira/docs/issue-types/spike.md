<!-- Generated by Claude Sonnet 4.5 (Anthropic) - December 2025 -->

# Spike Guide

Guide on Jira Spikes: time-boxed research and investigation to reduce uncertainty before implementation.

## Usage

```bash
# Create a spike (uses common/spike.yaml template by default)
/jira:create spike OCPBUGS "Investigate database performance bottleneck"

# Override with a specific template
/jira:create spike OCPBUGS "Investigate performance" --template common/spike

# OCPEDGE spike (uses project-specific template)
/jira:create spike OCPEDGE "Research adaptive topology implementation"
```

**Templates:** For more information on templates, see the [Template Guide](../../templates/README.md).

---

## What is a Spike?

A spike is:
- **Time-boxed research** - Investigation with fixed duration
- **Uncertainty reduction** - Answers questions before committing to implementation
- **Knowledge gathering** - Explores feasibility, options, or approaches
- **Proof of concept** - Creates minimal prototype to validate ideas
- **Decision support** - Provides information needed to make technical decisions

### Spike Characteristics

Spikes should:
- Have **clear research questions** to answer
- Be **time-boxed** (typically 1-3 days, max 1 sprint)
- **Produce documentation** - findings, recommendations, and follow-up work
- **Reduce uncertainty** - Provide information to make decisions
- **Lead to action** - Results in follow-up stories/tasks or decision not to proceed

## Spike vs Task vs Story

### When to Use a Spike

Use a **Spike** when:
- **Outcome is uncertain** - Don't know if approach will work
- **Need to research** - Evaluating options or technologies
- **Feasibility unknown** - Need to prove it's possible
- **Creating proof of concept** - Minimal prototype to validate idea
- **Answering technical questions** - Investigation required before planning

**Examples of spikes:**
- "Investigate eBPF for network policy enforcement"
- "Research options for multi-cluster metrics aggregation"
- "Proof of concept: Serverless edge workload deployment"
- "Evaluate performance impact of switching to etcd v3.6"
- "Investigate root cause of intermittent timeout in health checks"

### When to Use a Task Instead

Use a **Task** when:
- You know what to do (just need to do it)
- Implementation approach is clear
- No research or investigation needed
- Work is technical but straightforward

**Example:**
- Spike: "Investigate whether we can migrate to etcd v3.6" (uncertain)
- Task: "Upgrade etcd from v3.5 to v3.6" (known approach)

### When to Use a Story Instead

Use a **Story** when:
- Delivering user-facing functionality
- Implementation approach is known
- No investigation phase needed

**Example:**
- Spike: "Investigate feasibility of autoscaling for edge clusters"
- Story: "As a cluster admin, I want to enable autoscaling on edge clusters"

### Comparison Table

| Aspect | Spike | Task | Story |
|--------|-------|------|-------|
| **Purpose** | Research, investigation | Technical implementation | User functionality |
| **Outcome** | Documentation, decision | Code/config changes | Working feature |
| **Uncertainty** | High (that's why we spike) | Low (approach known) | Low (requirements clear) |
| **Duration** | Time-boxed (1-3 days) | Days to weeks | 1 sprint |
| **Deliverable** | Findings, recommendation | Implementation | User value |
| **Follow-up** | Creates stories/tasks | Completes work | May inform future work |

## Spike Components

### 1. Summary

Brief description of what needs to be investigated (5-10 words)

**Examples:**
- ✅ "Investigate eBPF for network policy enforcement"
- ✅ "Research multi-cluster metrics aggregation options"
- ✅ "Evaluate performance impact of etcd v3.6 upgrade"
- ✅ "Proof of concept: serverless edge workload deployment"

### 2. Overview

What needs to be investigated and why:
- Context for the investigation
- Problem or opportunity being explored
- Why this research is needed

**Example:**
```text
Investigate whether eBPF can replace iptables for network policy enforcement
on edge devices. Current iptables-based approach has performance limitations
on resource-constrained edge hardware. eBPF may provide better performance
but requires investigation to validate feasibility.
```

### 3. Research Questions

Specific questions the spike should answer:
- What do we need to know?
- What decisions depend on these answers?
- What are the key unknowns?

**Example:**
```text
Research Questions:
* Can eBPF handle our network policy requirements?
* What's the performance difference vs iptables on ARM processors?
* What's the minimum kernel version required?
* Are there limitations or edge cases that would prevent adoption?
* What's the implementation complexity and risk?
```

### 4. Involved Teams

Which teams participate or are affected:
- Who should be consulted?
- Who needs the results?
- Who will work on follow-up implementation?

**Example:**
```text
Involved Teams:
* MicroShift (implementation)
* Networking (policy requirements)
* Performance (benchmarking)
```

### 5. Acceptance Criteria

What deliverables define the spike as complete:

**Typical AC for spikes:**
- Document findings with evidence
- Provide recommendation (proceed/don't proceed/modify approach)
- Create follow-up stories/tasks if proceeding
- Share findings with stakeholders

**Example:**
```text
Acceptance Criteria:
* Document findings with performance benchmarks
* Provide recommendation: proceed, investigate alternatives, or stay with iptables
* Create follow-up epic if recommendation is to proceed
* Present findings to MicroShift and Networking teams
```

## Best Practices

### 1. Clear Research Questions

**Good research questions:**
- Specific and focused
- Answerable within time box
- Actionable (answers inform decisions)

**Example:**
```text
✅ "Can eBPF handle our network policy requirements?"
   (Specific, testable, actionable)

❌ "Is eBPF good?"
   (Too vague, not actionable)
```

### 2. Time-Boxed

**Guidelines:**
- **Short spike:** 1-2 days for simple investigation
- **Medium spike:** 3-5 days for proof of concept
- **Maximum:** 1 sprint (5 days)

**If spike needs more time:**
- Break into smaller spikes
- Create as Epic with child spikes
- Consider if it's actually a Story

### 3. Document Findings

**Required documentation:**
- Summary of investigation
- Answers to research questions
- Evidence (benchmarks, prototypes, test results)
- Recommendation with justification
- Follow-up work identified

**Example structure:**
```text
Spike Findings: eBPF Network Policy Investigation

Summary:
eBPF can replace iptables for network policy enforcement with 40% performance
improvement on ARM processors. Implementation is feasible but requires kernel 5.7+.

Research Question Answers:
1. Can eBPF handle our requirements? YES - all current policies translatable
2. Performance difference? 40% faster on ARM, 25% faster on x86
3. Minimum kernel version? 5.7 (available in RHEL 9.0+)
4. Limitations? None found for our use cases
5. Implementation complexity? Moderate - estimated 2 sprints

Recommendation: PROCEED with eBPF migration

Evidence:
- Benchmark results attached: ebpf-benchmarks.pdf
- Proof of concept code: https://github.com/org/repo/tree/spike-ebpf
- Test coverage: All 47 existing network policies successfully translated

Follow-up Work:
- Epic created: OCPEDGE-2500 "Migrate to eBPF network policy enforcement"
- Child stories will implement migration in phases
```

### 4. Leads to Action

**Spike outcomes:**
- **Proceed:** Create follow-up stories/epics for implementation
- **Don't proceed:** Document why and close
- **Need more info:** Create another spike with refined questions
- **Alternative approach:** Document and create stories for alternative

**Never:** Leave spike without clear outcome and next steps

### 5. Share Findings

**Communication:**
- Share with stakeholders and involved teams
- Present findings in team meeting if significant
- Update related epics/stories with outcomes
- Link spike from follow-up work

### 6. Small and Focused

**Good spike:**
- Answers 2-5 specific questions
- Can complete in days, not weeks
- Has clear boundaries

**Too large:**
- Tries to answer 20 questions
- Would take multiple sprints
- Scope keeps expanding

**Fix:** Break into multiple focused spikes or create as Epic

## Anti-Patterns to Avoid

### ❌ Spike Becomes Implementation

**Example:**
```text
Spike: "Investigate eBPF for network policy"

Outcome: Fully implemented eBPF network policy in production
```

**Why bad:** Spikes are for research, not implementation. No time to write tests, documentation, handle edge cases.

**Fix:** Spike documents findings, creates follow-up stories for implementation
```text
Spike outcome: Document that eBPF is feasible
Follow-up: Story "Implement eBPF network policy with full test coverage"
```

---

### ❌ No Time Box

**Example:**
```text
Spike: "Research best practices for cluster management"

Duration: 3 months, still ongoing
```

**Why bad:** Spikes should reduce uncertainty quickly, not become endless research

**Fix:** Set strict time box (1-5 days), narrow scope
```text
Spike: "Evaluate top 3 cluster management tools against our requirements"
Time box: 3 days
```

---

### ❌ Vague Research Questions

**Example:**
```text
Research Questions:
* Is this a good idea?
* Will it work?
* Should we do it?
```

**Why bad:** Too vague, not testable, not actionable

**Fix:** Be specific and concrete
```text
Research Questions:
* Can eBPF handle our 47 existing network policy configurations?
* What's the performance difference on ARM processors (measured in packet/sec)?
* What's the minimum kernel version required, and do our platforms support it?
```

---

### ❌ No Recommendation

**Example:**
```text
Spike outcome: "We investigated eBPF. Here are some facts about it. <end>"
```

**Why bad:** No decision made, uncertainty not reduced, spike wasted

**Fix:** Always include clear recommendation
```text
Spike outcome:
Findings: <evidence>
Recommendation: Proceed with eBPF migration
Rationale: 40% performance gain, low risk, moderate effort (2 sprints)
Next steps: Epic PROJ-500 created for implementation
```

---

### ❌ Spike Used to Delay Decision

**Example:**
```text
Manager: "Should we proceed with approach A or B?"
Team: "Let's create a spike to investigate"
Manager: "We've already researched this extensively"
Team: "Yeah but... another spike won't hurt"
```

**Why bad:** Using spikes to avoid making decisions, analysis paralysis

**Fix:** Only spike when genuine uncertainty exists
```text
If sufficient information exists → Make decision
If uncertainty exists → Spike to gather specific information
If avoiding decision → Address root cause (fear of failure, unclear requirements, etc.)
```

---

### ❌ Spike Without Follow-Up

**Example:**
```text
Spike: Completed, findings documented
Follow-up: <none created>
Status: Closed

3 months later: No one remembers what we learned or what to do next
```

**Why bad:** Spike results lost, investigation wasted

**Fix:** Always create follow-up work or explicit "don't proceed" decision
```text
Spike outcome: eBPF is feasible
Follow-up: Epic PROJ-500 "eBPF Migration" created with 8 child stories
Link: Spike linked to epic
```

## Examples

### Example 1: Technology Evaluation Spike

**Summary:** "Evaluate performance impact of etcd v3.6 upgrade"

**Description:**
```text
Evaluate the performance impact of upgrading from etcd v3.5 to v3.6 on
control plane clusters. Need to understand if upgrade provides performance
benefits and identify any risks or breaking changes.

**Research Questions**

* What's the performance difference (latency, throughput) on our workloads?
* Are there breaking API or configuration changes?
* What's the upgrade process and can we rollback if needed?
* Are there known issues or regressions in v3.6?
* What's the resource usage difference (CPU, memory, disk)?

**Involved Teams**

* Control Plane
* Performance Engineering
* SRE

**Acceptance Criteria**

* Performance benchmarks comparing v3.5 vs v3.6 on representative workloads
* Document any breaking changes and required configuration updates
* Provide recommendation with risk assessment
* Create follow-up stories if proceeding with upgrade
```

---

### Example 2: Feasibility Spike

**Summary:** "Investigate feasibility of serverless edge workload deployment"

**Description:**
```text
Investigate whether serverless workload deployment (e.g., Knative, OpenFaaS)
is feasible on resource-constrained edge clusters. Current VM-based approach
has high overhead on edge hardware.

**Research Questions**

* Can serverless platforms run on edge hardware (2 CPU, 4GB RAM)?
* What's the cold-start latency on edge devices?
* What's the resource overhead vs traditional deployments?
* Can we support our required scale (100 functions per edge cluster)?
* What's the minimum cluster size required?

**Involved Teams**

* MicroShift
* Edge Computing
* Serverless

**Acceptance Criteria**

* Proof of concept running on edge hardware with benchmarks
* Document resource requirements and limitations
* Provide recommendation: serverless, optimize current approach, or investigate alternatives
* Create epic with stories if recommendation is to proceed
* Present findings to MicroShift and Edge teams
```

---

### Example 3: Root Cause Investigation Spike

**Summary:** "Investigate root cause of intermittent etcd timeouts"

**Description:**
```text
Investigate the root cause of intermittent etcd health check timeouts
affecting 5% of clusters. Timeouts cause false unhealthy status and
trigger unnecessary alerts.

**Research Questions**

* What's the root cause of timeouts (network, resource contention, etcd performance)?
* Why only 5% of clusters? What's different about affected clusters?
* Are timeouts correlated with specific events (scaling, upgrades, high load)?
* Is the 2-second timeout too aggressive for our environment?
* Can we reproduce in test environment?

**Involved Teams**

* Control Plane
* SRE
* Networking

**Acceptance Criteria**

* Root cause identified with supporting evidence (metrics, logs, traces)
* Provide recommendation: fix root cause, adjust timeout, or both
* Create bug or stories for fix implementation
* Document findings in investigation summary
```

## Spike Workflow

### Before the Spike

1. **Define research questions** - What do we need to know?
2. **Set time box** - How long to investigate? (1-5 days max)
3. **Identify stakeholders** - Who needs the results?
4. **Establish success criteria** - What deliverables define "done"?

### During the Spike

1. **Focus on questions** - Stay focused on research questions
2. **Document as you go** - Record findings, not just at the end
3. **Collect evidence** - Benchmarks, prototypes, test results
4. **Communicate progress** - Keep stakeholders informed
5. **Respect time box** - Stop at deadline even if more to learn

### After the Spike

1. **Document findings** - Write clear summary with evidence
2. **Make recommendation** - Proceed, don't proceed, or alternative approach
3. **Create follow-up work** - Stories, epics, or bugs as needed
4. **Share results** - Present to team and stakeholders
5. **Link dependencies** - Connect spike to follow-up work
6. **Close spike** - Mark complete with clear outcome

## Related

- [Task Guide](task.md) - Creating tasks for known technical work
- [Story Guide](story.md) - Creating user stories after spike reduces uncertainty
- [Epic Guide](epic.md) - Creating epics that may require spikes to de-risk
- [Template Reference](../../templates/common/spike.yaml) - Spike template used by `/jira:create spike`
