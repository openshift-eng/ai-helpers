# Pattern Analyzer

**Minimal data gathering + Claude AI intelligence**

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Python: search_repos.py                 ‚îÇ
‚îÇ  ‚Ä¢ GitHub Code Search API (paginated)    ‚îÇ
‚îÇ  ‚Ä¢ Deduplicates file matches             ‚îÇ
‚îÇ  ‚Ä¢ Ranks repos by quality                ‚îÇ
‚îÇ  ‚Üì                                        ‚îÇ
‚îÇ  Output: repos.json                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Python: Parallel git clone (8 workers)  ‚îÇ
‚îÇ  ‚Ä¢ --depth 1 --filter=blob:none          ‚îÇ
‚îÇ  ‚Üì                                        ‚îÇ
‚îÇ  Output: repos/ + analysis.log           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Claude AI:                              ‚îÇ
‚îÇ  ‚Ä¢ Read repos.json                       ‚îÇ
‚îÇ  ‚Ä¢ Explore cloned repos                  ‚îÇ
‚îÇ  ‚Ä¢ Analyze user's project                ‚îÇ
‚îÇ  ‚Ä¢ Generate ANALYSIS.md                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Features:**
- ‚úÖ **Parallel cloning:** Up to 8 concurrent clones (3-5x faster!)
- ‚úÖ **Pagination:** Fetches ALL search results (up to 1000)
- ‚úÖ **Deduplication:** Removes duplicate file matches
- ‚úÖ **All file types:** Searches Dockerfiles, shell scripts, YAML, Go, etc.
- ‚úÖ **Smart caching:** Uses `analysis.log` as 7-day cache marker

## Quick Usage

```bash
# Navigate to YOUR project first!
cd ~/your-operator-project

# Locate the analyzer script
ANALYZER_SCRIPT=$(find ~ -name "analyze_pattern.sh" -path "*/pattern-analyzer/*" 2>/dev/null | head -1)

# Run data gathering
$ANALYZER_SCRIPT \
  "NetworkPolicy" \
  --repos 50

# Or if ai-helpers is in workspace:
# plugins/openshift/skills/pattern-analyzer/analyze_pattern.sh "NetworkPolicy" --repos 50
```

**Output:**
```
‚úÖ DATA GATHERING COMPLETE

Generated data:
  ‚Ä¢ repos.json - 50 repositories metadata
  ‚Ä¢ repos/ - 50 cloned repositories
  ‚Ä¢ analysis.log - Execution log

ü§ñ NEXT: Claude AI Analysis

Claude should now:
  1. READ repos.json
  2. EXPLORE cloned repos
  3. ANALYZE your project
  4. GENERATE detailed recommendations
  5. CREATE ANALYSIS.md with complete guide
```

## What You Get

**Claude generates:** `.work/design-patterns/<pattern>/ANALYSIS.md`

Contains:
- Statistical insights from 50 repos
- Real code examples (extracted from repos)
- Specific recommendations for YOUR project
- Complete struct definitions
- Full implementation code
- Step-by-step integration guide
- Testing examples
- References to similar repos

## Scripts

| Script | Purpose | Key Features |
|--------|---------|--------------|
| `analyze_pattern.sh` | Orchestration | Cache management, clone coordination |
| `search_repos.py` | GitHub search | **Pagination**, deduplication, ranking |

**Only 2 scripts!** Everything else is Claude AI.

**What makes search_repos.py powerful:**
- Paginates through all GitHub search results (not just first 100)
- Deduplicates file matches across pages
- Searches all file types by default (optional language filter)
- Ranks repos by stars, activity, and relevance

## Arguments

```bash
./analyze_pattern.sh <pattern> [options]
```

- `<pattern>` - Pattern name or search term
  - Examples: "NetworkPolicy", "/usr/bin/gather", "must-gather"
- `--orgs <orgs>` - GitHub orgs (default: openshift,kubernetes)
- `--repos <N>` - Max repos (default: 50, range: 3-50)
- `--language <LANG>` - Language filter (default: all)
  - Examples: `go`, `python`, `shell`, `dockerfile`
  - Omit to search all file types
- `--refresh` - Force refresh (ignore cache)
- `--skip-clone` - Use existing cloned repos

## Examples

### Example 1: Go Type Pattern
```bash
cd ~/my-operator
./analyze_pattern.sh "NetworkPolicy" --language go --repos 50
```

### Example 2: Shell Script Pattern (all file types)
```bash
cd ~/must-gather-operator
./analyze_pattern.sh "/usr/bin/gather" --repos 50
# Searches: Dockerfiles, shell scripts, YAML, Makefiles, etc.
```

### Example 3: Quick Analysis
```bash
./analyze_pattern.sh "ValidatingWebhook" --repos 10
```

### Example 4: Use Cached Data
```bash
./analyze_pattern.sh "ProxyConfig" --skip-clone
# Uses existing repos/, re-runs search only
```

**After data gathering completes:**
```bash
# Claude creates comprehensive analysis
cat .work/design-patterns/<pattern>/ANALYSIS.md
```

## Why This Works

**Python is good at:**
- GitHub API calls
- git clone operations
- File system operations

**Claude is good at:**
- Reading and understanding code
- Finding patterns and similarities
- Generating context-specific recommendations
- Writing complete, working implementations
- Explaining reasoning

**Use the right tool for each job!** üéØ

## Output Structure

```
.work/design-patterns/<pattern>/
‚îú‚îÄ‚îÄ repos.json          # Metadata (from Python)
‚îú‚îÄ‚îÄ analysis.log        # Execution log (from bash)
‚îú‚îÄ‚îÄ ANALYSIS.md         # Complete guide (from Claude) ‚≠ê
‚îî‚îÄ‚îÄ repos/              # Cloned repos (from git)
    ‚îú‚îÄ‚îÄ cluster-network-operator/
    ‚îú‚îÄ‚îÄ api/
    ‚îú‚îÄ‚îÄ sdn/
    ‚îî‚îÄ‚îÄ ... (50 repos)
```

## Requirements

- Python 3.6+
- Git
- ~400-600MB disk space for 50 repos (optimized shallow clones with `--depth 1`)
- (Optional) GITHUB_TOKEN for higher API limits
